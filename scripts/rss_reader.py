import feedparser
import json
import os
from datetime import datetime, timezone
import time

# -------------------
# Configuration
# -------------------
SEEN_JSON_PATH = "state/seen.json"
PAPERS_JSON_PATH = "output/papers.json"

# åœ°çƒç§‘å­¦ç›¸å…³ RSS æºåˆ—è¡¨
RSS_URLS = [
    "http://www.nature.com/nature/current_issue/rss",  #nature
    "https://www.science.org/action/showFeed?type=etoc&feed=rss&jc=science",  #science
    "https://www.science.org/action/showFeed?type=etoc&feed=rss&jc=sciadv",   #SA
    "https://www.nature.com/ngeo.rss",     #NG
    "https://www.nature.com/ncomms.rss",   #NC
    "https://www.nature.com/natrevearthenviron.rss",   #Nature REE
    "https://www.nature.com/commsenv.rss",             #NC EE
    "https://www.pnas.org/action/showFeed?type=searchTopic&taxonomyCode=topic&tagCode=earth-sci", #PNAS
    "https://www.annualreviews.org/rss/content/journals/earth/latestarticles?fmt=rss", #Annual Review of Earth and Planetary Sciences
    "https://rss.sciencedirect.com/publication/science/00128252",   #ESR
    "https://rss.sciencedirect.com/publication/science/0012821X",   #EPSL
    "https://agupubs.onlinelibrary.wiley.com/feed/19448007/most-recent",  #GRL
    "https://agupubs.onlinelibrary.wiley.com/feed/15252027/most-recent",   #Geochemistry, Geophysics, Geosystems
    "https://agupubs.onlinelibrary.wiley.com/feed/21699356/most-recent",    #JGR:Solid Earth
    "https://rss.sciencedirect.com/publication/science/00167037",     #GCA
    "https://pubs.geoscienceworld.org/rss/site_65/advanceAccess_33.xml",   #Geology
    "https://pubs.geoscienceworld.org/rss/site_119/advanceAccess_60.xml",  #AM
    "https://pubs.geoscienceworld.org/economicgeology/issue/120/6",       #EG
    "https://pubs.geoscienceworld.org/rss/site_69/advanceAccess_35.xml", #GSAB
    "https://academic.oup.com/rss/site_5332/advanceAccess_3198.xml",     #NSR
    "https://agupubs.onlinelibrary.wiley.com/feed/19449194/most-recent",  #Tectonic
    "https://www.earthsciencefrontiers.net.cn/CN/rss_zxly_1005-2321.xml",#geoscience frontiers  åœ°å­¦å‰ç¼˜ï¼ˆè‹±æ–‡ç‰ˆï¼‰
]

def load_seen_papers():
    """Load the list of seen paper IDs from a JSON file."""
    if os.path.exists(SEEN_JSON_PATH):
        with open(SEEN_JSON_PATH, "r", encoding="utf-8") as f:
            try:
                # å°è¯•åŠ è½½æ•´ä¸ªåˆ—è¡¨ï¼Œç„¶åæå– ID é›†åˆ
                seen_list = json.load(f)
                return {p.get("id") for p in seen_list if isinstance(p, dict) and p.get("id")}, seen_list
            except json.JSONDecodeError:
                print("Warning: seen.json is corrupted or empty. Starting fresh.")
                return set(), []
    return set(), []

def save_seen_papers(seen_list):
    """Save the updated list of seen papers to a JSON file."""
    os.makedirs(os.path.dirname(SEEN_JSON_PATH), exist_ok=True)
    with open(SEEN_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(seen_list, f, indent=2, ensure_ascii=False)

def parse_date(entry):
    """
    Attempts to parse the publication date from the RSS entry.
    
    Prioritizes published_parsed/updated_parsed, falls back to today's date.
    Returns date in 'YYYY-MM-DD' format.
    """
    try:
        # Use updated_parsed or published_parsed (struct_time format)
        date_struct = entry.get('updated_parsed') or entry.get('published_parsed')
        if date_struct:
            # Convert struct_time to datetime object and then format
            date_dt = datetime(*date_struct[:6], tzinfo=timezone.utc)
            return date_dt.strftime("%Y-%m-%d")
    except Exception as e:
        # If parsing fails, print a warning and fall back to today
        print(f"Warning: Failed to parse date for entry '{entry.get('title', 'Unknown')}'. Error: {e}")
        pass

    # Fallback to today's date (Local time zone)
    return datetime.now().strftime("%Y-%m-%d")

def fetch_new_entries():
    """Fetches new entries from all RSS feeds."""
    
    seen_ids, seen_list = load_seen_papers()
    new_entries_list = []
    
    print(f"Loaded {len(seen_ids)} existing paper IDs.")
    
    for url in RSS_URLS:
        print(f"è§£æ RSS: {url}")
        try:
            feed = feedparser.parse(url)
            source_name = feed.feed.get('title', url.split('/')[2])
            
            for entry in feed.entries:
                # Use 'id' or 'link' as a unique identifier (ID is usually better)
                uid = entry.get("id") or entry.get("link")
                
                if not uid:
                    continue # Skip entries without a usable ID
                
                # 1. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if uid in seen_ids:
                    continue # Skip if already seen

                # 2. å¦‚æœæ˜¯æ–°æ–‡ç« ï¼Œåˆ™è®°å½•
                
                # Clean up summary (remove HTML/CDATA tags often left by feedparser)
                summary_raw = entry.get('summary', entry.get('content', [{}])[0].get('value'))
                summary_text = summary_raw.replace('<p>', '').replace('</p>', '').replace('<br>', '').strip() if summary_raw else ""
                
                # Extract authors
                authors_list = [author.get('name') for author in entry.get('authors', [])]
                
                new_entry = {
                    "id": uid,
                    "title": entry.get('title', 'Unknown Title'),
                    "link": entry.get('link', ''),
                    "authors": authors_list,
                    "summary": summary_text,
                    "source": source_name,
                    # ğŸš€ ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨æ–‡ç« æœ¬èº«çš„å‘å¸ƒæ—¥æœŸ
                    "date": parse_date(entry) 
                    "sent": False
                }
                
                new_entries_list.append(new_entry)
                seen_ids.add(uid) # Add to the seen set immediately

        except Exception as e:
            print(f"Error processing RSS feed {url}: {e}")
            
    # Combine old seen list with new entries
    # Note: We must update the seen list to include the newly fetched data structure
    
    # é‡æ–°æ„å»º seen_list: ç¡®ä¿åªåŒ…å«å”¯ä¸€çš„ã€æœ€æ–°çš„æ¡ç›®
    # å› ä¸ºæˆ‘ä»¬åªç”¨æ–°çš„ entry å¯¹è±¡å¡«å…… new_entries_listï¼Œæ‰€ä»¥åªéœ€å°†æ–°æ—§åˆ—è¡¨åˆå¹¶
    
    # ğŸš¨ æ³¨æ„ï¼šä¸ºäº†ä¿è¯ seen_list ä¸ä¼šæ— é™å¢é•¿ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦é™åˆ¶å…¶å¤§å°ã€‚
    # å‡è®¾æˆ‘ä»¬ä¿ç•™ 500000 ç¯‡å†å²æ–‡ç« ã€‚
    MAX_HISTORY = 500000000
    
    # è¿‡æ»¤æ‰æ—§åˆ—è¡¨ä¸­é‡å¤çš„ IDï¼Œç„¶ååˆå¹¶
    # ç¡®ä¿ seen_list åªåŒ…å«é‚£äº› ID ä¸åœ¨ new_entries_list ä¸­çš„æ—§æ¡ç›®
    current_seen_ids = {p['id'] for p in new_entries_list}
    filtered_old_seen = [p for p in seen_list if p.get('id') not in current_seen_ids]
    
    updated_seen_list = new_entries_list + filtered_old_seen
    
    # è£å‰ªåˆ—è¡¨ä»¥é™åˆ¶å¤§å°
    updated_seen_list = updated_seen_list[:MAX_HISTORY]
    
    save_seen_papers(updated_seen_list)
    
    return new_entries_list

if __name__ == "__main__":
    new_papers = fetch_new_entries()
    
    print(f"å…±æŠ“å– {len(new_papers)} ç¯‡æ–°è®ºæ–‡")

    if new_papers:
        # Save new papers to a separate file (optional, but helpful for debugging/future features)
        os.makedirs(os.path.dirname(PAPERS_JSON_PATH), exist_ok=True)
        with open(PAPERS_JSON_PATH, "w", encoding="utf-8") as f:
            json.dump(new_papers, f, indent=2, ensure_ascii=False)
        print(f"æ–°è®ºæ–‡è¯¦æƒ…å·²ä¿å­˜è‡³ {PAPERS_JSON_PATH}")
    else:
        print("ä»Šæ—¥æ— æ–°å¢è®ºæ–‡ã€‚")
